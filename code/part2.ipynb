{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c2e4d9-76a0-4f82-98a4-9fee9ab4c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:16:48.071000: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-27 20:16:48.130131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 20:16:49.045315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(27.4154, device='cuda:0'), [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                        | 1/10 [44:55<6:44:22, 2695.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for one epoch with batch size 64: 2695.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [7:22:33<00:00, 2655.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-11257.4043, device='cuda:0'), [6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def make_data_point(sent):\n",
    "    \"\"\"\n",
    "        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n",
    "        dic['tokens'] = Tokens padded with <START> and <STOP>\n",
    "        dic['pos'] = POS tags padded with <START> and <STOP>\n",
    "        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
    "        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n",
    "    :param sent: String.  The input CoNLL format string\n",
    "    :return: Dict from String to Array of Strings.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
    "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
    "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
    "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
    "    return dic\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n",
    "    :param filename: String\n",
    "    :return: Array of dictionaries.  Each dictionary has the format returned by the make_data_point function.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = []\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "        \n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, char_embedding_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(10, char_embedding_dim)\n",
    "        self.char_cnn = nn.Conv2d(in_channels=1, out_channels=char_embedding_dim, kernel_size=(1, char_embedding_dim))\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size).to(forward_var.device)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1).to(forward_var.device)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]].to(forward_var.device)\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    def get_char_indices(self, word_idx):\n",
    "        \"\"\"\n",
    "        Extracts character indices using nltk.word_tokenize.\n",
    "        \"\"\"\n",
    "        word = train_data[word_idx]['tokens']  # Assuming 'tokens' key holds the word\n",
    "        char_indices = [word_to_ix[char] for char in word]  # Assuming word_to_ix exists\n",
    "        return char_indices\n",
    "\n",
    "    def _get_lstm_features_cnn(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        sentence = sentence.to(self.word_embeds.weight.device)\n",
    "\n",
    "        # Character Embeddings (assuming you have a character embedding layer)\n",
    "        char_embeddings = self.char_embedding.to(sentence.device)  # Assuming `self.char_embedding` exists\n",
    "\n",
    "        # Get character indices for each word (replace with your logic)\n",
    "        char_ids = []\n",
    "        for word_idx in sentence:\n",
    "            # Convert word index to a list of character indices based on your vocabulary\n",
    "            chars = self.get_char_indices(word_idx)# Your logic to get character indices from word index\n",
    "            char_ids.append(torch.tensor(chars).to(sentence.device)) \n",
    "\n",
    "        # Pad sequences to have the same length (optional, but recommended)\n",
    "        char_ids = pad_sequence(char_ids, batch_first=True, padding_value=0)  # 0 for padding\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=char_embeddings.num_embeddings,  # Assuming embedding dim\n",
    "                            out_channels=11,  # Adjust as needed\n",
    "                            kernel_size=4,  # Experiment with kernel size\n",
    "                            padding=1)  # Pad to maintain input size\n",
    "\n",
    "        # Pass character embeddings through CNN\n",
    "        cnn_out = self.conv1(char_embeddings(char_ids))\n",
    "        \n",
    "\n",
    "        lstm_out = torch.max(F.relu(cnn_out), dim=2)[0]  # ReLU activation and max pooling\n",
    "\n",
    "        # Reshape for LSTM\n",
    "        lstm_out = lstm_out.view(len(sentence), -1)\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_out, self.hidden = self.lstm(lstm_out, self.hidden)\n",
    "\n",
    "        # Final output\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim * 2)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "\n",
    "        return lstm_feats\n",
    "\n",
    "    def _get_lstm_features(self, sentence): \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        sentence = sentence.to(self.word_embeds.weight.device)\n",
    "        \n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        \n",
    "        self.hidden = tuple(h.to(embeds.device) for h in self.hidden)\n",
    "        \n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
    "        \n",
    "        \n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1, device=feats.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000., device=feats.device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "\n",
    "\n",
    "train_data = read_data('ner.train')  # Assume this returns a list of dictionaries\n",
    "dev_data = read_data('ner.dev')\n",
    "test_data = read_data('ner.test')\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 40\n",
    "HIDDEN_DIM = 40\n",
    "CHAR_EMBEDDING_DIM = 4\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(set([word for sentence in train_data + dev_data + test_data for word in sentence['tokens']]))}\n",
    "tag_to_ix = {tag: i for i, tag in enumerate(set([tag for sentence in train_data + dev_data + test_data for tag in sentence['gold_tags']]))}\n",
    "\n",
    "# Model, optimizer and loss initialization\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(train_data[0]['tokens'], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in train_data[0]['gold_tags']], dtype=torch.long).to(device)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "def generate_minibatches(training_data, batch_size):\n",
    "    random.shuffle(training_data)\n",
    "    minibatches = []\n",
    "    for i in range(0, len(training_data), batch_size):\n",
    "        minibatch = training_data[i:i + batch_size]\n",
    "        minibatches.append(minibatch)\n",
    "    return minibatches\n",
    "\n",
    "\n",
    "def make_data_point(sent):\n",
    "    \"\"\"\n",
    "        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n",
    "        dic['tokens'] = Tokens padded with <START> and <STOP>\n",
    "        dic['pos'] = POS tags padded with <START> and <STOP>\n",
    "        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
    "        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n",
    "    :param sent: String.  The input CoNLL format string\n",
    "    :return: Dict from String to Array of Strings.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
    "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
    "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
    "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
    "    return dic\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "batch_size = 64\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(10)):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    minibatches = generate_minibatches(train_data, batch_size)\n",
    "    for i, minibatch in enumerate(minibatches):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentences_in = [prepare_sequence(sentence['tokens'], word_to_ix) for sentence in minibatch]\n",
    "        sentences_in = pad_sequence(sentences_in, batch_first=True)\n",
    "        targets = [torch.tensor([tag_to_ix[t] for t in sentence['gold_tags']], dtype=torch.long) for sentence in minibatch]\n",
    "        targets = pad_sequence(targets, batch_first=True)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = 0\n",
    "        for sentence_in, target in zip(sentences_in, targets):\n",
    "            loss += model.neg_log_likelihood(sentence_in, target)\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(minibatches) + i)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    if epoch < 1:\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training time for one epoch with batch size {batch_size}: {elapsed_time:.2f} seconds\")\n",
    "    torch.save(model.state_dict(), f'bilstm_crf_model_epoch_{epoch}.pth')\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(train_data[0]['tokens'], word_to_ix).to(device)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "output_file = \"dev_predictions.txt\"\n",
    "loaded_model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM)\n",
    "loaded_model.load_state_dict(torch.load('bilstm_crf_model_epoch_9.pth'))\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Open a file to write the predicted tags\n",
    "with open('dev_predictions.txt', 'w') as f:\n",
    "    # Check predictions on dev data after training\n",
    "    with torch.no_grad():\n",
    "        for example in dev_data:\n",
    "            # Prepare the sequence from the current example in dev data\n",
    "            dev_sent = prepare_sequence(example['tokens'], word_to_ix).to(device)\n",
    "            # Pass the sequence to the model for prediction\n",
    "            _, predicted_tags = loaded_model(dev_sent)\n",
    "            \n",
    "            # Convert predicted tags from tensor to a list of tag strings\n",
    "            predicted_tags = [list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tag_id)] for tag_id in predicted_tags]\n",
    "            \n",
    "            # Write the predicted tags to the file\n",
    "            f.write(' '.join(predicted_tags) + '\\n')\n",
    "\n",
    "output_file = \"test_predictions.txt\"\n",
    "\n",
    "# Open a file to write the predicted tags\n",
    "with open('test_predictions.txt', 'w') as f:\n",
    "    # Check predictions on dev data after training\n",
    "    with torch.no_grad():\n",
    "        for example in test_data:\n",
    "            # Prepare the sequence from the current example in dev data\n",
    "            dev_sent = prepare_sequence(example['tokens'], word_to_ix).to(device)\n",
    "            \n",
    "            # Pass the sequence to the model for prediction\n",
    "            _, predicted_tags = loaded_model(dev_sent)\n",
    "            \n",
    "            # Convert predicted tags from tensor to a list of tag strings\n",
    "            predicted_tags = [list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tag_id)] for tag_id in predicted_tags]\n",
    "            \n",
    "            # Write the predicted tags to the file\n",
    "            f.write(' '.join(predicted_tags) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb13601-e271-47fd-8cdd-f261e948cbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7303628572335515, 0.7140596222866256, 0.7213724655391789)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_iob_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into tags and filter out empty strings\n",
    "            tags = line.strip().split()\n",
    "            if tags:  # only add non-empty lists\n",
    "                data.append(tags)\n",
    "    return data\n",
    "\n",
    "# Using the function to read a file and store data in a list\n",
    "# Note: Replace 'your_file.txt' with your actual file name\n",
    "file_path = './dev_predictions.txt'\n",
    "iob_tags = read_iob_file(file_path)\n",
    "\n",
    "gold_tags_list=[]\n",
    "for example in dev_data:\n",
    "    gold_tags_list.append(example['gold_tags'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def calculate_metrics(gold, pred):\n",
    "    # Flatten the list of tags into a single list for each set\n",
    "    gold_flat = [item for sublist in gold for item in sublist]\n",
    "    pred_flat = [item for sublist in pred for item in sublist]\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        gold_flat, pred_flat, average='macro', labels=['I-PER', 'I-LOC', 'I-ORG']\n",
    "    )\n",
    "\n",
    "    return precision, recall, f1\n",
    "calculate_metrics(gold_tags_list,iob_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5282487-7e1d-4072-9cc6-dabe4189adca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6551379235768838, 0.6066772753114434, 0.6286521666352259)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_iob_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into tags and filter out empty strings\n",
    "            tags = line.strip().split()\n",
    "            if tags:  # only add non-empty lists\n",
    "                data.append(tags)\n",
    "    return data\n",
    "\n",
    "# Using the function to read a file and store data in a list\n",
    "# Note: Replace 'your_file.txt' with your actual file name\n",
    "file_path = './test_predictions.txt'\n",
    "iob_tags = read_iob_file(file_path)\n",
    "\n",
    "gold_tags_list=[]\n",
    "for example in test_data:\n",
    "    gold_tags_list.append(example['gold_tags'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def calculate_metrics(gold, pred):\n",
    "    # Flatten the list of tags into a single list for each set\n",
    "    gold_flat = [item for sublist in gold for item in sublist]\n",
    "    pred_flat = [item for sublist in pred for item in sublist]\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        gold_flat, pred_flat, average='macro', labels=['I-PER', 'I-LOC', 'I-ORG']\n",
    "    )\n",
    "\n",
    "    return precision, recall, f1\n",
    "calculate_metrics(gold_tags_list,iob_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
