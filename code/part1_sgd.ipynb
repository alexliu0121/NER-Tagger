{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyeebuhZdJb1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "import os\n",
    "import sys\n",
    "print(os.listdir('/content/gdrive/My Drive/NLP-202-A1-Code'))\n",
    "sys.path.append('/content/gdrive/My Drive/NLP-202-A1-Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazetteer_file(file_path):\n",
    "    gazetteer = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Ensure the line is not empty\n",
    "                parts = line.strip().split(' ', 1)  # Split the line into two parts\n",
    "                if len(parts) == 2:\n",
    "                    tag, entities = parts\n",
    "                    entities_list=entities.split(' ')\n",
    "                    for entity in entities_list:\n",
    "                        if entity not in gazetteer:\n",
    "                            gazetteer[entity] = []\n",
    "                        gazetteer[entity].append(tag)\n",
    "                else:\n",
    "                    print(f\"Invalid line format: {line}\")\n",
    "    return gazetteer\n",
    "\n",
    "# Example usage\n",
    "file_path = './gazetteer.txt'  # Change to your actual file path\n",
    "gazetteer = read_gazetteer_file(file_path)\n",
    "# print(gazetteer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CHdbUbDpdTBp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data\n",
      "Training...\n",
      "Training done\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dot_product'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 546\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfdict[txt[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(txt[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# best_f1=0\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m \u001b[43mmain_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# Uncomment to train a model (need to implement 'sgd' function)\u001b[39;00m\n\u001b[1;32m    547\u001b[0m main_predict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner.dev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Uncomment to predict on 'dev.ner' using the model 'model' (need to implement 'decode' function)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 422\u001b[0m, in \u001b[0;36mmain_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    421\u001b[0m dev_data \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner.dev\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 422\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m test_data \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner.test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    424\u001b[0m evaluate(test_data, parameters, feature_names, tagset)\n",
      "Cell \u001b[0;32mIn[20], line 337\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data, parameters, feature_names, tagset)\u001b[0m\n\u001b[1;32m    335\u001b[0m     all_gold_tags\u001b[38;5;241m.\u001b[39mextend(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgold_tags\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# deletes <START> and <STOP>\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     input_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 337\u001b[0m     all_predicted_tags\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;66;03m# deletes <START> and <STOP>\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conllevaluate(all_gold_tags, all_predicted_tags)\n",
      "Cell \u001b[0;32mIn[20], line 261\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(inputs, input_len, parameters, feature_names, tagset)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(cur_tag, pre_tag, i):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mdot_product(features\u001b[38;5;241m.\u001b[39mcompute_features(cur_tag, pre_tag, i))\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 44\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(input_length, tagset, score)\u001b[0m\n\u001b[1;32m     41\u001b[0m backpointers \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tagset) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_length)]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tagset):\n\u001b[0;32m---> 44\u001b[0m     viterbi[\u001b[38;5;241m1\u001b[39m][s] \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<START>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, input_length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tagset):\n",
      "Cell \u001b[0;32mIn[20], line 259\u001b[0m, in \u001b[0;36mpredict.<locals>.score\u001b[0;34m(cur_tag, pre_tag, i)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(cur_tag, pre_tag, i):\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_product\u001b[49m(features\u001b[38;5;241m.\u001b[39mcompute_features(cur_tag, pre_tag, i))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dot_product'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from conlleval import evaluate as conllevaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def decode(input_length, tagset, score):\n",
    "    \"\"\"\n",
    "    Compute the highest scoring sequence according to the scoring function.\n",
    "    :param input_length: int. number of tokens in the input including <START> and <STOP>\n",
    "    :param tagset: Array of strings, which are the possible tags.  Does not have <START>, <STOP>\n",
    "    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n",
    "        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n",
    "    :return: Array strings of length input_length, which is the highest scoring tag sequence including <START> and <STOP>\n",
    "    \"\"\"\n",
    "    # Look at the function compute_score for an example of how the tag sequence should be scored\n",
    "    \n",
    "    # Initialize Viterbi matrix and back pointers\n",
    "    # viterbi = np.zeros((input_length, len(tagset)))\n",
    "    # backpointers = np.zeros((input_length, len(tagset)), dtype=int)\n",
    "    \n",
    "    # # Initialize the start probabilities\n",
    "    # viterbi[0, :] = score('<START>', None, 0)\n",
    "    \n",
    "    # # Viterbi algorithm\n",
    "    # for i in range(1, input_length):\n",
    "    #     for j, tag in enumerate(tagset):\n",
    "    #         scores = [viterbi[i-1, k] + score(tag, tagset[k], i) for k in range(len(tagset))]\n",
    "    #         viterbi[i, j] = max(scores)\n",
    "    #         backpointers[i, j] = np.argmax(scores)\n",
    "    \n",
    "    # # Backtrack to find the best sequence\n",
    "    # best_path = []\n",
    "    # best_tag_index = np.argmax(viterbi[input_length-1, :])\n",
    "    # for i in range(input_length-1, 0, -1):\n",
    "    #     best_path.insert(0, tagset[best_tag_index])\n",
    "    #     best_tag_index = backpointers[i, best_tag_index]\n",
    "    \n",
    "    # # Return the best path, with <START> and <STOP> tags\n",
    "    # return ['<START>'] + best_path + ['<STOP>']\n",
    "    viterbi = [[0] * len(tagset) for _ in range(input_length)]\n",
    "    backpointers = [[0] * len(tagset) for _ in range(input_length)]\n",
    "\n",
    "    for s, tag in enumerate(tagset):\n",
    "        viterbi[1][s] = score(tag, '<START>',1) \n",
    "\n",
    "    for i in range(2, input_length-1):\n",
    "        for s, tag in enumerate(tagset):\n",
    "            max_score = float('-inf')\n",
    "            max_prev_tag = None\n",
    "            for s_prev, prev_tag in enumerate(tagset):\n",
    "                score_i = viterbi[i - 1][s_prev] + score(tag, prev_tag, i)\n",
    "                if score_i > max_score:\n",
    "                    max_score = score_i\n",
    "                    max_prev_tag = s_prev\n",
    "            viterbi[i][s] = max_score\n",
    "            backpointers[i][s] = max_prev_tag\n",
    "\n",
    "    best_seq = [''] * input_length\n",
    "    max_last_score = float('-inf')\n",
    "    max_last_tag = None\n",
    "    for s, tag in enumerate(tagset):\n",
    "        score_last = viterbi[input_length - 1][s] + score('<STOP>', tag, input_length - 1)\n",
    "        if score_last > max_last_score:\n",
    "            max_last_score = score_last\n",
    "            max_last_tag = s\n",
    "\n",
    "    best_seq[input_length - 1] = tagset[max_last_tag]\n",
    "    current_tag = max_last_tag\n",
    "    for i in range(input_length - 2, 0, -1):  \n",
    "        current_tag_index = backpointers[i + 1][current_tag] \n",
    "        best_seq[i] = tagset[current_tag_index]  \n",
    "        current_tag = current_tag_index  \n",
    "\n",
    "    best_seq[0] = '<START>'\n",
    "    best_seq[-1] = '<STOP>'\n",
    "    return best_seq\n",
    "\n",
    "def compute_score(tag_seq, input_length, score):\n",
    "    \"\"\"\n",
    "    Computes the total score of a tag sequence\n",
    "    :param tag_seq: Array of String of length input_length. The tag sequence including <START> and <STOP>\n",
    "    :param input_length: Int. input length including the padding <START> and <STOP>\n",
    "    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n",
    "        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for i in range(1, input_length):\n",
    "        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def compute_features(tag_seq, input_length, features):\n",
    "    \"\"\"\n",
    "    Compute f(xi, yi)\n",
    "    :param tag_seq: [tags] already padded with <START> and <STOP>\n",
    "    :param input_length: input length including the padding <START> and <STOP>\n",
    "    :param features: func from token index to FeatureVector\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    feats = FeatureVector({})\n",
    "    for i in range(1, input_length):\n",
    "        feats.times_plus_equal(1, features.compute_features(tag_seq[i], tag_seq[i - 1], i))\n",
    "    return feats\n",
    "\n",
    "    # Examples from class (from slides Jan 15, slide 18):\n",
    "    # x = will to fight\n",
    "    # y = NN TO VB\n",
    "    # features(x,y) =\n",
    "    #  {\"wi=will^yi=NN\": 1, // \"wi=\"+current_word+\"^yi=\"+current_tag\n",
    "    # \"yi-1=START^yi=NN\": 1,\n",
    "    # \"ti=to+^yi=TO\": 1,\n",
    "    # \"yi-1=NN+yi=TO\": 1,\n",
    "    # \"xi=fight^yi=VB\": 1,\n",
    "    # \"yi-1=TO^yi=VB\": 1}\n",
    "\n",
    "    # x = will to fight\n",
    "    # y = NN TO VBD\n",
    "    # features(x,y)=\n",
    "    # {\"wi=will^yi=NN\": 1,\n",
    "    # \"yi-1=START^yi=NN\": 1,\n",
    "    # \"ti=to+^yi=TO\": 1,\n",
    "    # \"yi-1=NN+yi=TO\": 1,\n",
    "    # \"xi=fight^yi=VBD\": 1,\n",
    "    # \"yi-1=TO^yi=VBD\": 1}\n",
    "\n",
    "def sgd(training_size, epochs, gradient, parameters, training_observer):\n",
    "    \"\"\"\n",
    "    (This should be SSGD)\n",
    "    Stochastic gradient descent\n",
    "    :param training_size: int. Number of examples in the training set\n",
    "    :param epochs: int. Number of epochs to run SGD for\n",
    "    :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
    "    :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
    "    :param training_observer: func that takes epoch and parameters.  You can call this function at the end of each\n",
    "           epoch to evaluate on a dev set and write out the model parameters for early stopping.\n",
    "    :return: final parameters\n",
    "    \"\"\"\n",
    "    # Look at the FeatureVector object.  You'll want to use the function times_plus_equal to update the\n",
    "    # parameters.\n",
    "    # To implement early stopping you can call the function training_observer at the end of each epoch.\n",
    "    step_size = 1  # Fixed step size as mentioned\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(training_size):\n",
    "            # Calculate the gradient for the current example\n",
    "            grad = gradient(i)\n",
    "            # Update the parameters\n",
    "            parameters.times_plus_equal(-step_size, grad)\n",
    "        # Optional: Call the training observer at the end of each epoch\n",
    "        # to evaluate and possibly perform early stopping\n",
    "        f1_score = training_observer(epoch, parameters)\n",
    "        global best_f1\n",
    "        if f1_score > best_f1:\n",
    "            best_f1 = f1_score\n",
    "            # Save the best model\n",
    "            parameters.write_to_file('best_model')\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def adagrad(training_size, epochs, gradient, parameters, training_observer):\n",
    "    \"\"\"\n",
    "    :param training_size: int. Number of examples in the training set\n",
    "    :param epochs: int. Number of epochs to run SGD for\n",
    "    :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
    "    :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
    "    :param training_observer: func that takes epoch and parameters.  You can call this function at the end of each\n",
    "           epoch to evaluate on a dev set and write out the model parameters for early stopping.\n",
    "    :return: final parameters\n",
    "    \"\"\"\n",
    "    # Look at the FeatureVector object.  You'll want to use the function times_plus_equal to update the\n",
    "    # parameters.\n",
    "    # To implement early stopping you can call the function training_observer at the end of each epoch.\n",
    "    temp = FeatureVector({})\n",
    "    best_stop_f1 = 0\n",
    "    best_stop = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in tqdm.tqdm(range(training_size)):\n",
    "            temp.power_plus_equal(2, gradient(i))\n",
    "            parameters.times_plus_equal(-1, gradient(i).sqrt_div_equal(temp))\n",
    "        f1 = training_observer(epoch, parameters)\n",
    "        if f1 > best_stop_f1:  \n",
    "            best_stop = epoch\n",
    "\n",
    "    print(best_stop)\n",
    "\n",
    "    return parameters\n",
    "    \n",
    "\n",
    "def train(data, feature_names, tagset, epochs):\n",
    "    \"\"\"\n",
    "    Trains the model on the data and returns the parameters\n",
    "    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n",
    "        make_data_point function).\n",
    "    :param feature_names: Array of Strings.  The list of feature names.\n",
    "    :param tagset: Array of Strings.  The list of tags.\n",
    "    :param epochs: Int. The number of epochs to train\n",
    "    :return: FeatureVector. The learned parameters.\n",
    "    \"\"\"\n",
    "    parameters = FeatureVector({})   # creates a zero vector\n",
    "\n",
    "    def perceptron_gradient(i):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the Perceptron loss for example i\n",
    "        :param i: Int\n",
    "        :return: FeatureVector\n",
    "        \"\"\"\n",
    "        inputs = data[i]\n",
    "        input_len = len(inputs['tokens'])\n",
    "        gold_labels = inputs['gold_tags']\n",
    "        features = Features(inputs, feature_names)\n",
    "\n",
    "        def score(cur_tag, pre_tag, i):\n",
    "            return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "\n",
    "        tags = decode(input_len, tagset, score)\n",
    "        fvector = compute_features(tags, input_len, features)           # Add the predicted features\n",
    "        #print('Input:', inputs)        # helpful for debugging\n",
    "        #print(\"Predicted Feature Vector:\", fvector.fdict)\n",
    "        #print(\"Predicted Score:\", parameters.dot_product(fvector)) # compute_score(tags, input_len, score)\n",
    "        fvector.times_plus_equal(-1, compute_features(gold_labels, input_len, features))    # Subtract the features for the gold labels\n",
    "        #print(\"Gold Labels Feature Vector: \", compute_features(gold_labels, input_len, features).fdict)\n",
    "        #print(\"Gold Labels Score:\", parameters.dot_product(compute_features(gold_labels, input_len, features)))\n",
    "        return fvector\n",
    "\n",
    "    def training_observer(epoch, parameters):\n",
    "        \"\"\"\n",
    "        Evaluates the parameters on the development data, and writes out the parameters to a 'model.iter'+epoch and\n",
    "        the predictions to 'ner.dev.out'+epoch.\n",
    "        :param epoch: int.  The epoch\n",
    "        :param parameters: Feature Vector.  The current parameters\n",
    "        :return: Double. F1 on the development data\n",
    "        \"\"\"\n",
    "        dev_data = read_data('ner.dev')\n",
    "        (_, _, f1) = evaluate(dev_data, parameters, feature_names, tagset)\n",
    "\n",
    "        # change output directory\n",
    "        write_predictions('./output/SGD/ner.dev.out'+str(epoch), dev_data, parameters, feature_names, tagset)\n",
    "        parameters.write_to_file('./output/SGD/model.iter'+str(epoch))\n",
    "        return f1\n",
    "\n",
    "    # return sgd(len(data), epochs, perceptron_gradient, parameters, training_observer)\n",
    "        return adagrad(len(data), epochs, perceptron_gradient, parameters, training_observer)\n",
    "\n",
    "\n",
    "def predict(inputs, input_len, parameters, feature_names, tagset):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param inputs:\n",
    "    :param input_len:\n",
    "    :param parameters:\n",
    "    :param feature_names:\n",
    "    :param tagset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features = Features(inputs, feature_names)\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "\n",
    "    return decode(input_len, tagset, score)\n",
    "\n",
    "\n",
    "def make_data_point(sent):\n",
    "    \"\"\"\n",
    "        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n",
    "        dic['tokens'] = Tokens padded with <START> and <STOP>\n",
    "        dic['pos'] = POS tags padded with <START> and <STOP>\n",
    "        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
    "        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n",
    "    :param sent: String.  The input CoNLL format string\n",
    "    :return: Dict from String to Array of Strings.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
    "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
    "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
    "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
    "    return dic\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n",
    "    :param filename: String\n",
    "    :return: Array of dictionaries.  Each dictionary has the format returned by the make_data_point function.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = []\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data\n",
    "\n",
    "def write_predictions(out_filename, all_inputs, parameters, feature_names, tagset):\n",
    "    \"\"\"\n",
    "    Writes the predictions on all_inputs to out_filename, in CoNLL 2003 evaluation format.\n",
    "    Each line is token, pos, NP_chuck_tag, gold_tag, predicted_tag (separated by spaces)\n",
    "    Sentences are separated by a newline\n",
    "    The file can be evaluated using the command: python conlleval.py < out_file\n",
    "    :param out_filename: filename of the output\n",
    "    :param all_inputs:\n",
    "    :param parameters:\n",
    "    :param feature_names:\n",
    "    :param tagset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(out_filename, 'w', encoding='utf-8') as f:\n",
    "        for inputs in all_inputs:\n",
    "            input_len = len(inputs['tokens'])\n",
    "            tag_seq = predict(inputs, input_len, parameters, feature_names, tagset)\n",
    "            for i, tag in enumerate(tag_seq[1:-1]):  # deletes <START> and <STOP>\n",
    "                f.write(' '.join([inputs['tokens'][i+1], inputs['pos'][i+1], inputs['NP_chunk'][i+1], inputs['gold_tags'][i+1], tag])+'\\n') # i + 1 because of <START>\n",
    "            f.write('\\n')\n",
    "\n",
    "def evaluate(data, parameters, feature_names, tagset):\n",
    "    \"\"\"\n",
    "    Evaluates precision, recall, and F1 of the tagger compared to the gold standard in the data\n",
    "    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n",
    "        make_data_point function)\n",
    "    :param parameters: FeatureVector.  The model parameters\n",
    "    :param feature_names: Array of Strings.  The list of features.\n",
    "    :param tagset: Array of Strings.  The list of tags.\n",
    "    :return: Tuple of (prec, rec, f1)\n",
    "    \"\"\"\n",
    "    all_gold_tags = [ ]\n",
    "    all_predicted_tags = [ ]\n",
    "    for inputs in data:\n",
    "        all_gold_tags.extend(inputs['gold_tags'][1:-1])  # deletes <START> and <STOP>\n",
    "        input_len = len(inputs['tokens'])\n",
    "        all_predicted_tags.extend(predict(inputs, input_len, parameters, feature_names, tagset)[1:-1]) # deletes <START> and <STOP>\n",
    "    return conllevaluate(all_gold_tags, all_predicted_tags)\n",
    "\n",
    "def test_decoder():\n",
    "    # See https://classes.soe.ucsc.edu/nlp202/Winter21/assignments/A1_Debug_Example.pdf\n",
    "    \n",
    "    tagset = ['NN', 'VB']     # make up our own tagset\n",
    "\n",
    "    def score_wrap(cur_tag, pre_tag, i):\n",
    "        retval = score(cur_tag, pre_tag, i)\n",
    "        print('Score('+cur_tag+','+pre_tag+','+str(i)+') returning '+str(retval))\n",
    "        return retval\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        if i == 0:\n",
    "            print(\"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\")\n",
    "        if i == 1:\n",
    "            if pre_tag != '<START>':\n",
    "                print(\"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"+pre_tag)\n",
    "            if cur_tag == 'NN':\n",
    "                return 6\n",
    "            if cur_tag == 'VB':\n",
    "                return 4\n",
    "        if i == 2:\n",
    "            if cur_tag == 'NN' and pre_tag == 'NN':\n",
    "                return 4\n",
    "            if cur_tag == 'NN' and pre_tag == 'VB':\n",
    "                return 9\n",
    "            if cur_tag == 'VB' and pre_tag == 'NN':\n",
    "                return 5\n",
    "            if cur_tag == 'VB' and pre_tag == 'VB':\n",
    "                return 0\n",
    "        if i == 3:\n",
    "            if cur_tag != '<STOP>':\n",
    "                print('ERROR: Current tag at i = 3 should be <STOP>. Current tag = '+cur_tag)\n",
    "            if pre_tag == 'NN':\n",
    "                return 1\n",
    "            if pre_tag == 'VB':\n",
    "                return 1\n",
    "\n",
    "    predicted_tag_seq = decode(4, tagset, score_wrap)\n",
    "    print('Predicted tag sequence should be = <START> VB NN <STOP>')\n",
    "    print('Predicted tag sequence = '+' '.join(predicted_tag_seq))\n",
    "    print(\"Score of ['<START>','VB','NN','<STOP>'] = \"+str(compute_score(['<START>','VB','NN','<STOP>'], 4, score)))\n",
    "    print('Max score should be = 14')\n",
    "    print('Max score = '+str(compute_score(predicted_tag_seq, 4, score)))\n",
    "\n",
    "\n",
    "def main_predict(data_filename, model_filename):\n",
    "    \"\"\"\n",
    "    Main function to make predictions.\n",
    "    Loads the model file and runs the NER tagger on the data, writing the output in CoNLL 2003 evaluation format to data_filename.out\n",
    "    :param data_filename: String\n",
    "    :param model_filename: String\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    data = read_data(data_filename)\n",
    "    parameters = FeatureVector({})\n",
    "    parameters.read_from_file(model_filename)\n",
    "\n",
    "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "    feature_names = ['tag', 'prev_tag', 'current_word','len_k','gazetteer','capi']\n",
    "\n",
    "    write_predictions('./output/SGD/'data_filename+'.out', data, parameters, feature_names, tagset)\n",
    "    evaluate(data, parameters, feature_names, tagset)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def main_train():\n",
    "    \"\"\"\n",
    "    Main function to train the model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print('Reading training data')\n",
    "    train_data = read_data('ner.train')\n",
    "    #train_data = read_data('ner.train')[1:1] # if you want to train on just one example\n",
    "\n",
    "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "    feature_names = ['tag', 'prev_tag', 'current_word','len_k','gazetteer','capi']\n",
    "\n",
    "    print('Training...')\n",
    "    parameters = train(train_data, feature_names, tagset, epochs=10)\n",
    "    print('Training done')\n",
    "    dev_data = read_data('ner.dev')\n",
    "    evaluate(dev_data, parameters, feature_names, tagset)\n",
    "    test_data = read_data('ner.test')\n",
    "    evaluate(test_data, parameters, feature_names, tagset)\n",
    "    parameters.write_to_file('model_sgd')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "class Features(object):\n",
    "    def __init__(self, inputs, feature_names):\n",
    "        \"\"\"\n",
    "        Creates a Features object\n",
    "        :param inputs: Dictionary from String to an Array of Strings.\n",
    "            Created in the make_data_point function.\n",
    "            inputs['tokens'] = Tokens padded with <START> and <STOP>\n",
    "            inputs['pos'] = POS tags padded with <START> and <STOP>\n",
    "            inputs['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
    "            inputs['gold_tags'] = DON'T USE! The gold tags padded with <START> and <STOP>\n",
    "        :param feature_names: Array of Strings.  The list of features to compute.\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.gazetteer = gazetteer\n",
    "\n",
    "    def compute_features(self, cur_tag, pre_tag, i):\n",
    "        \"\"\"\n",
    "        Computes the local features for the current tag, the previous tag, and position i\n",
    "        :param cur_tag: String.  The current tag.\n",
    "        :param pre_tag: String.  The previous tag.\n",
    "        :param i: Int. The position\n",
    "        :return: FeatureVector\n",
    "        \"\"\"\n",
    "        token_i=self.inputs['tokens'][i]\n",
    "        feats = FeatureVector({})\n",
    "        # if 'tag' in self.feature_names:\n",
    "        #     feats.times_plus_equal(1, FeatureVector({'t='+cur_tag: 1}))\n",
    "        # if 'prev_tag' in self.feature_names:\n",
    "        #     feats.times_plus_equal(1, FeatureVector({'ti='+cur_tag+\"+ti-1=\"+pre_tag: 1}))\n",
    "        # if 'current_word' in self.feature_names:\n",
    "        #     feats.times_plus_equal(1, FeatureVector({f't='+cur_tag+'+w='+self.inputs['tokens'][i]: 1}))\n",
    "        if 'tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({f't={cur_tag}': 1}))\n",
    "        if 'prev_tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({f'ti={cur_tag}+ti-1={pre_tag}': 1}))\n",
    "        if 'current_word' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({f't={cur_tag}+w={token_i}': 1}))\n",
    "        if 'shape' in self.feature_names:\n",
    "            shape = ''.join(['A' if c.isupper() else 'a' if c.islower() else 'd' if c.isdigit() else c for c in self.inputs['tokens']])\n",
    "            feats.times_plus_equal(1, FeatureVector({f'si={shape}+ti={cur_tag}': 1.0}))\n",
    "        # for k in range(1, 5):  # Prefixes of length 1 to 4\n",
    "        #     if len(self.inputs['tokens']) >= k:\n",
    "        #         prefix = self.inputs['tokens'][:k]\n",
    "        #         feats.times_plus_equal(1, FeatureVector({f'PRE{k}i={prefix}+ti={cur_tag}': 1.0}))\n",
    "        if 'len_k' in self.feature_names:\n",
    "            for k in range(len(self.inputs['tokens'][i][:4])):\n",
    "                feats.times_plus_equal(1, FeatureVector({'prei='+self.inputs['tokens'][i][:k]+'+t='+cur_tag: 1}))\n",
    "        # if 'gazetteer' in self.feature_names and self.inputs['tokens'][i] in gazetteer and gazetteer[self.inputs['tokens'][i]] == cur_tag.split('-')[1]:\n",
    "        #     feats.times_plus_equal(1, FeatureVector({f'GAZ=True+t={cur_tag}': 1.0}))\n",
    "        if 'gazetteer' in self.feature_names:\n",
    "            if self.inputs['tokens'][i] in self.gazetteer.keys() and self.gazetteer[self.inputs['tokens'][i]] == cur_tag: #'LOC': #and has LOC tag:\n",
    "                feats.times_plus_equal(1, FeatureVector({'gazi='+\"True\"+'+t='+cur_tag: 1}))\n",
    "        # if 'capitalization' in self.feature_names and self.inputs['tokens'][0].isupper():\n",
    "        #     feats.times_plus_equal(1, FeatureVector({f'CAPi=True+ti={cur_tag}': 1.0}))\n",
    "        #     # feats.times_plus_equal(1, FeatureVector({'capi='+\"True\"+\"+ti=\"+cur_tag: 1}))\n",
    "        if 'capi' in self.feature_names:\n",
    "            if self.inputs['tokens'][i][0].isupper():\n",
    "                feats.times_plus_equal(1, FeatureVector({'capi='+\"True\"+'+t='+cur_tag: 1}))\n",
    "\n",
    "        return feats\n",
    "\n",
    "class FeatureVector(object):\n",
    "\n",
    "    def __init__(self, fdict):\n",
    "        self.fdict = fdict\n",
    "\n",
    "    def times_plus_equal(self, scalar, v2):\n",
    "        \"\"\"\n",
    "        self += scalar * v2\n",
    "        :param scalar: Double\n",
    "        :param v2: FeatureVector\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for key, value in v2.fdict.items():\n",
    "            self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n",
    "\n",
    "\n",
    "    def dot_product(self, v2):\n",
    "        \"\"\"\n",
    "        Computes the dot product between self and v2.  It is more efficient for v2 to be the smaller vector (fewer\n",
    "        non-zero entries).\n",
    "        :param v2: FeatureVector\n",
    "        :return: Int\n",
    "        \"\"\"\n",
    "        retval = 0\n",
    "        for key, value in v2.fdict.items():\n",
    "            retval += value * self.fdict.get(key, 0)\n",
    "        return retval\n",
    "\n",
    "    def write_to_file(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the feature vector to a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print('Writing to ' + filename)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for key, value in self.fdict.items():\n",
    "                f.write('{} {}\\n'.format(key, value))\n",
    "\n",
    "\n",
    "    def read_from_file(self, filename):\n",
    "        \"\"\"\n",
    "        Reads a feature vector from a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.fdict = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                txt = line.split()\n",
    "                self.fdict[txt[0]] = float(txt[1])\n",
    "\n",
    "# best_f1=0\n",
    "main_train()    # Uncomment to train a model (need to implement 'sgd' function)\n",
    "main_predict('ner.dev', 'model')  # Uncomment to predict on 'dev.ner' using the model 'model' (need to implement 'decode' function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7463 phrases; correct: 2062.\n",
      "accuracy:  40.15%; (non-O)\n",
      "accuracy:  82.89%; precision:  27.63%; recall:  34.85%; FB1:  30.82\n",
      "              LOC: precision:  64.17%; recall:  54.32%; FB1:  58.83  1549\n",
      "             MISC: precision:  47.11%; recall:  33.92%; FB1:  39.44  658\n",
      "              ORG: precision:  55.56%; recall:  17.52%; FB1:  26.64  423\n",
      "              PER: precision:  10.82%; recall:  28.55%; FB1:  15.69  4833\n"
     ]
    }
   ],
   "source": [
    "main_predict('ner.dev', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5616 phrases; found: 7439 phrases; correct: 1654.\n",
      "accuracy:  35.74%; (non-O)\n",
      "accuracy:  80.17%; precision:  22.23%; recall:  29.45%; FB1:  25.34\n",
      "              LOC: precision:  59.45%; recall:  52.88%; FB1:  55.97  1482\n",
      "             MISC: precision:  30.40%; recall:  25.11%; FB1:  27.50  579\n",
      "              ORG: precision:  50.52%; recall:  11.90%; FB1:  19.26  388\n",
      "              PER: precision:   8.04%; recall:  25.03%; FB1:  12.17  4990\n"
     ]
    }
   ],
   "source": [
    "main_predict('ner.test', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.822122571001497"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oInyKo1pfTW2"
   },
   "source": [
    "To sort the model weights for easy viewing, you can use Unix commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHEtAu_LfmPG"
   },
   "outputs": [],
   "source": [
    "!cat \"/content/gdrive/My Drive/NLP-202-A1-Code/model\" | awk '{print $2, $1}' | sort -gr > \"/content/gdrive/My Drive/NLP-202-A1-Code/model.sorted.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bS4d4Acfp1S"
   },
   "source": [
    "The file `model.sorted.txt` will be viewable in your Google Drive folder."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-202_A1_Code.ipynb",
   "provenance": [
    {
     "file_id": "186hCS3cdEtl0vpkgCXHYgLUu-BkFZZ9T",
     "timestamp": 1583157228957
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
